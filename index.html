<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Fluorescence Diffraction Tomography using Explicit Neural Fields">
  <meta name="keywords" content="Tomography, Neural Fields">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fluorescence Diffraction Tomography using Explicit Neural Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
 
    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon2.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<!--   <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .column.content img {
      width: 100%;
      height: auto;
    }
    .caption {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text .section-title {
      font-weight: bold;
      font-style: italic;
    }
    .scientific-notation {
      font-family: 'Times New Roman', Times, serif;
      font-style: italic;
    }
    .superscript {
      vertical-align: super;
      font-size: smaller;
    }
  </style>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fluorescence Diffraction Tomography using Explicit Neural Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Renzhi He</a>,</span>
            <span class="author-block">
              <a href="">Yucheng Li</a>,</span>
            <span class="author-block">
              <a href="">Junjie Chen</a>,</span>
            <span class="author-block">
              <a href="https://cobi.ucdavis.edu/people">Yi Xue*</a>,
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Davis,</span>
<!--             <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2407.16657"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2407.16657"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cubhe/fdt_code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">



  
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Simultaneous imaging of fluorescence-labeled and label-free phase objects in the same sample can provide distinct and complementary information. Most multimodal fluorescence-phase imaging operates in transmission mode, capturing fluorescence images and phase images separately or sequentially, which limits their practical use for in vivo applications. Alternatively, simultaneous fluorescence-phase imaging in reflection mode, which captures diffracted fluorescence and then reconstructs phase information from fluorescence images, has been demonstrated with fluorescent beads and label-free single-layer cells. However, reconstructing the 3D refractive index (RI) of thick samples from fluorescence images over a large volume and at high resolution remains challenging.
          </p>
          <p>  
            To tackle this challenge, we develop fluorescence diffraction tomography (FDT) with explicit neural fields to reconstruct the 3D RI of thick samples from diffracted fluorescence images captured on a defocused image plane. The successful reconstruction of 3D RI using FDT relies on four key components: coarse-to-fine structure, self-calibration, a differential multi-slice rendering model, and partial coherent masks. The explicit representation integrates with coarse-to-fine structure for high-speed, high-resolution reconstruction, while the differential multi-slice rendering model enables self-calibration of system parameters, ensuring accurate forward image prediction and RI reconstruction. Partial coherent masks efficiently resolve discrepancies between the coherent light model and partial coherent light data.
          </p>
          <p>  
            FDT successfully reconstructed the RI of 3D cultured label-free 3D MuSCs tube in a 530 x 530 x 300 µm<span class="superscript">3</span> volume
            at 1024 x 1024 pixels across 24 z-layers from fluorescence images, demonstrating high fidelity 3D RI reconstruction of bulky and heterogeneous 
            biological samples in vitro.
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Overview</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/fig_overview3.jpg" alt="Matting Example" style="width: 100%; height: auto;">
            <p class="caption">
              <strong>Overview of FDT using explicit neural fields.</strong>
            </p>
            <p>
              <span class="caption">a</span>, The coarse-to-fine structure represents the unknown refractive index (RI) with neural fields and resolves it through three stages of increasing resolution as the number of iterations increases. 
              <span class="caption">b</span>, Self-calibration is applied to localize the fluorophore positions, starting from an initial estimation by Gaussian fitting. The positions are then set as iterative parameters and optimized during the training process. 
              <span class="caption">c</span>, The rendering equation is based on a differential multi-slice model, which takes two inputs: the RI from <span class="caption">a</span> and the fluorophore positions from <span class="caption">b</span>. The model calculates the light field as it is modulated by the heterogeneous RI on each slice using the Born approximation. Fresnel propagation is used to calculate light propagation between slices. 
              <span class="caption">d</span>, A partially coherent light mask is generated by computing the light field on the image plane without the heterogeneous RI and then binarizing the light field to create the partially coherent mask. The masks are applied to both the predicted and measured images. The masked images are used to calculate the loss function, incorporating L1, L2, SSIM, and regularization terms.
            </p>

          </div>
        </div>
      </div>
      
      <style>
        .column.content img {
          width: 100%;
          height: auto;
        }
      </style>
    </div>

    <!--/ Matting. -->



<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Results</h2>

    <div class="content has-text-justified">
      <p>
        We verify the validation of our model as well as each component in the
        model under the simulation 'ucdavis' data. Then, we applied the FDT to experimental
        data of thin (MDCK) and thick (3D muscle tube) sample to show the effectiveness of our model for variety of
        structure and z-section ability. 
      </p>
      <div class="related-work">
        <h3 class="title is-4">UCDavis</h3>
        <div class="work-item">
          <a href="">
            <img src="./static/images/fig_ucd3.jpg" alt="Progressive Encoding for Neural Optimization">
          </a>
          <p class="caption">
            <strong>Reconstruction of the 3D RI of a simulated "UCDavis" pattern.</strong>
          </p>
          <p>
            <span class="caption">a</span>, Reconstructed 3D RI distribution of the "UCDavis" pattern in 3D from 400 fluorescence images. 
            <span class="caption">b</span>, A representative ground-truth (GT) image, generated using the multi-slice model with the ground-truth 3D RI, where the RI of the letters is 1.38 and the background is 1.33. 
            <span class="caption">c</span>, The predicted fluorescence image under the same illumination as <span class="caption">b</span> with the reconstructed 3D RI. 
            <span class="caption">d</span>, Zoomed-in views of the regions within the orange and green boxes in <span class="caption">b</span> and <span class="caption">c</span>, respectively. The SSIM and PSNR between the ground-truth and predicted images are 0.9994 and 56.9062, respectively. 
            <span class="caption">e</span>, Results of self-calibration of the positions of fluorescent sources at 50, 100, and 150 iterations. The blue circle indicates the irradiated region from the ground-truth position of the excited fluorophore, and the white dashed line indicates the irradiated region from the predicted position of the fluorophore. 
            <span class="caption">f</span>, The plot on the right shows the MSE loss between the self-calibrated and ground-truth fluorophore positions, converging to 0.001 within 500 iterations, indicating successful self-calibration of the positions of fluorescent sources. 
            <span class="caption">g</span>, Comparison of the ground-truth RI (top row) and the predicted RI (bottom row) on each <i>z</i>-plane, as indicated by the axis below. 
            <span class="caption">h</span>, Effect of the coarse-to-fine structure on reconstruction results on two different <i>z</i>-planes (top row, <i>z</i> = 65 μm; bottom row, <i>z</i> = 53 μm). The first three columns show results of the coarse-to-fine structure at different iterations (100, 200, and 300) with progressively increasing sampling grid at 128 × 128, 256 × 256, and 512 × 512 pixels. The last column shows the results after 300 iterations without the coarse-to-fine structure at a sampling grid of 512 × 512 pixels. Comparing the third column and the fourth column, the coarse-to-fine structure mitigates crosstalk between <i>z</i>-planes and reconstructs the missing low-frequency signals.
          </p>
        </div>
        
        
        <div class="work-item">
          <h3 class="title is-4">MDCK</h3>
          <a href="">
            <img src="./static/images/fig_mdck3.jpg" alt="D-NeRF">
          </a>
<!--           <p class="caption">3D RI Reconstruction of a thin sample of live MDCK cells.</p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI distribution within a volume of 358.4 x 358.4 x 60 µm<span class="superscript">3</span> . 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images, with a highlighted region of interest. 
            <span class="caption">c</span>, Detailed views of the highlighted region: measured image (top), predicted image (middle), and error map (bottom). 
            <span class="caption">d-f</span>, Cross-sectional views of the RI difference  distribution along different planes (xy, xz, yz), illustrating spatial variations in RI. 
            <span class="caption">g</span>, Quantitative analysis of RI variation of the red box in <span class="caption">d</span>, showing RI along the X-axis (top) and Z-axis (bottom). 
            <span class="caption">h</span>, Sequence of images highlighting the appearance and disappearance of specific features within the region along the Z-axis.
          </p> -->
          </a>
          <p class="caption">
            <strong>3D RI reconstruction of a thin layer of live MDCK Cells sample.</strong>
          </p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI distribution of MDCK cells within a 358.4 × 358.4 × 44 μm<sup>3</sup> volume. The RI of the cells ranges between 1.33 to 1.36. 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images. 
            <span class="caption">c</span>, Zoomed-in views of the highlighted region: the measured image (top), the predicted image (middle), and the error map between the measured and predicted images (bottom). 
            <span class="caption">d</span>, Schematic diagram of the optical setup of FDT. Fluorescence is excited by scanning a focus with a spatial light modulator (SLM), and diffracted fluorescence images are captured in reflection mode using a camera. 
            <span class="caption">e-g</span>, Cross-sectional views of the RI distribution of MDCK cells on three representative planes that are 12.5 μm apart, showing optical sectioning ability and high 3D resolution. 
            <span class="caption">h</span>, Zoomed-in view of the image <i>z</i>-stack of cells in the highlighted region in <span class="caption">e</span>. The <i>z</i>-position of each image is labeled on the <i>z</i> axis below the images. The images again highlight the optical sectioning and high resolution of FDT.
          </p>
        </div>
        <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <h3 class="title is-4">2D view</h3>
  <!--           <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p> -->
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ri_mdck_z~1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Visual Effects. -->
  
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">3D view</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="dollyzoom" autoplay controls muted loop playsinline height="95%">
                <source src="./static/videos/ri_mdck_3d720.mp4"
                        type="video/mp4">
              </video>
            </div>
  
          </div>
        </div>
      </div>




        <p>
        </p>
        <p>
        </p>
        <div class="work-item">
          <h3 class="title is-4">3D tube</h3>
          <a href="">
            <img src="./static/images/fig_tube3.jpg" alt="NR-NeRF">
          </a>
          <p class="caption">
            <strong>3D RI reconstruction of a 3D cultured bovine myotube.</strong>
          </p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI of 3D cultured bovine myotube within a volume of 530 × 530 × 300 μm<sup>3</sup>. 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images. 
            <span class="caption">c-d</span>, Cross-sectional views of the reconstructed RI on two representative planes, showing high resolution and optical sectioning ability. 
            <span class="caption">e</span>, Zoomed-in details of the highlighted regions in <span class="caption">d</span>, labeled by corresponding colors, showing different morphologies of the 3D cultured bovine myotube during proliferation and differentiation. The results indicate that FDT can accurately reconstruct various structures across a wide range of spatial frequencies. See the video in the supplementary material for a better 3D visualization.
          </p>
        </div>

        <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h3 class="title is-4">Axial Rotational Perspective</h3>
  <!--           <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p> -->
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100% style="margin-top: 20px;">
              <source src="./static/videos/3dtube0721_11.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Visual Effects. -->
  
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">Radial Rotational Perspective</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="matting-video" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/3dtube0721_2.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        </div>
        
        <div class="columns is-centered">
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">Radial Rotational Perspective</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="matting-video" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/ri_3dtube_z~2.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        </div>
        
      </div>



        
      </div>
    </div>
  </div>
</div>

<style>
  .related-work {
    display: flex;
    flex-direction: column;
    align-items: center;
  }
  .work-item {
    text-align: center;
    margin-bottom: 20px;
    width: 100%;
  }
  .work-item img {
    display: block;
    margin: 0 auto;
    width: 100%;
    height: auto;
  }
  .caption {
    font-weight: bold;
    margin-top: 10px;
  }
</style>
</div>

<div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      
    
    <!-- Concurrent Work. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>

    <!-- Animation. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
<!--         <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        
<!--         <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p> -->
        
        <!--/ Re-rendering.

      </div>
    </div>
    <!--/ Animation. -->

  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{he2024fluorescencediffractiontomographyusing,
      title={Fluorescence Diffraction Tomography using Explicit Neural Fields}, 
      author={Renzhi He and Yucheng Li and Junjie Chen and Yi Xue},
      year={2024},
      eprint={2407.16657},
      archivePrefix={arXiv},
      primaryClass={physics.optics},
      url={https://arxiv.org/abs/2407.16657}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
